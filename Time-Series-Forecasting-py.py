# -*- coding: utf-8 -*-
"""Copy of PowerGen_CNN_FINALVerTUNSid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1frdvVPn8vYQkj7O0FumtC7yBhRSqqaeu

# **Time Series Forecasting**

 Change the path for input and output files and Run the cells one by one.

 You can directly skip to CNN part
"""

from google.colab import drive
drive.mount('/content/drive')

#Arima seasonal model

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import datetime
# load all data


from datetime import datetime
#We can use the read_csv() function to load the data and combine the first two columns into a single datetime column to use it as an index.
#dataset = pd.read_csv(r'/content/KMJ4Daily2018-2020.csv',index_col='Date',infer_datetime_format=True)
dataset = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/Data/KMJ4Daily2018-2020.csv',index_col='Date',infer_datetime_format=True)

dataset = dataset.dropna()
#print(dataset.shape)

dataset.index = pd.to_datetime(dataset.index)

lastDate = dataset.last_valid_index()

print(lastDate)
next = lastDate + pd.DateOffset(days=1)
print(next)
rng = pd.date_range(next, periods=31, freq='D')
means = dataset.mean()
idx = pd.Index(rng)
NewData = dataset.append(pd.DataFrame(index=idx))
NewData = NewData.fillna(means)
#NewData.tail(40)

#print(NewData.tail(35))

endDate = NewData.last_valid_index()
print("Predictions from :")
print(next ,  endDate)

# Commented out IPython magic to ensure Python compatibility.
from statsmodels.tsa.statespace.sarimax import SARIMAX
# %matplotlib inline

from scipy import stats


import statsmodels.api as sm



import pandas
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
from pandas.plotting import autocorrelation_plot
#from pandas.tools.plotting import autocorrelation_plot
from matplotlib import pyplot
q = NewData.resample('M').mean()
autocorrelation_plot(q)
pyplot.show() #p=20

plot_acf(NewData['Power Generation'])

plot_pacf(q['Power Generation']) #q val

#d = 1





sarimax_model = SARIMAX(NewData['Power Generation'],order=(20,1,4),seasonal_order=(1,1,1,30),exog=NewData['Temperature'])

res = sarimax_model.fit(maxiter=200,disp=False)

print(res.aic)

res.resid.plot(kind='kde')



res.summary()

res.resid.describe()

predictions = res.predict(start = next, end = endDate,exog = NewData['Temperature'], dynamic= True)

#print(pd1.tail(40))
pd1 = predictions.to_frame()
#print(pd1.tail(32))
#print(len(pd1),type(pd1))

pd1.columns = ['Pow_Prediction']
pd1.index = pd1.index.rename('Date')
#print(pd1.tail(31))
#pd1.tail(31).plot()
final1 = pd1.iloc[-31:,:]
#Output file
final1.to_excel(r"/content/drive/My Drive/Colab Notebooks/Output/CNN/TimeSeriesPred1.xlsx")



import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import datetime
# load all data
from keras.callbacks import ModelCheckpoint
# univariate multi-step vector-output 1d cnn model
from numpy import array
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.callbacks import EarlyStopping
from keras.models import load_model
# split a univariate sequence into samples
def split_sequence(sequence, n_steps_in, n_steps_out):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        # check if we are beyond the sequence
        if out_end_ix > len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

from keras import backend
def rmse(y_true, y_pred):
    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))

# define input sequence
#change file path
df = pd.read_csv(r'/content/KMJ4Daily2018-2020working.csv')

df = df.dropna()
raw_seq = df['Power Generation'].tolist()
# choose a number of time steps 365 is for input steps and 31 is number of days ahead
#n_steps_in, n_steps_out =365,31
n_steps_in, n_steps_out =365,31
# split into samples
X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features)) #shape[0] = total data, shape [1] = 365

# define model

"""#CNN Model run from here"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import datetime
# load all data
from keras.callbacks import ModelCheckpoint
# univariate multi-step vector-output 1d cnn model
from numpy import array
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.callbacks import EarlyStopping
from keras.models import load_model
# split a univariate sequence into samples
def split_sequence(sequence, n_steps_in, n_steps_out):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        # check if we are beyond the sequence
        if out_end_ix > len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

from keras import backend
def rmse(y_true, y_pred):
    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))

# define input sequence
#change file path
df = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/Data/KMJ4Daily2018-2020.csv')

df = df.dropna()

raw_seq = df['Power Generation'].tolist()


#print(raw_seq)

#raw_seq = df["Power Generation"].tolist()

#raw_seq = scaled_df1[0].tolist()

# choose a number of time steps 365 is for input steps and 31 is number of days ahead
#n_steps_in, n_steps_out =365,31
n_steps_in, n_steps_out =365,31
# split into samples
X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features)) #shape[0] = total data, shape [1] = 365

# define model

model = Sequential()


#the performance of the model can be optized by increasing the hidden layers  and epochs
#model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) #change the number of filters to tune. TUNING<<<<<<<<<<
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) #change the number of filters to tune. TUNING<<<<<<<<<< INPUT LAYER
model.add(MaxPooling1D(pool_size=2)) #add the max-pooling layer with MaxPooling1D() SECOND LAYER - POOLING layer
model.add(Flatten()) #normalize the shape of pooling layer
model.add(Dense(1000, activation='relu'))#relu activation function , change the dense to tune the model - add dense layer 4th layer

model.add(Dense(n_steps_out)) #output layer

# Stop the training once the monitor value are achieve and patiently train the model until number of epoch when there is still no improvement
#es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=80) #change the monitor parameter and patience to tune. TUNING<<<<<<<<<<<
es = EarlyStopping(monitor='val_loss', verbose=1, patience=100, mode='auto') #change the monitor parameter and patience to tune. TUNING<<<<<<<<<<<
#mc = ModelCheckpoint('/content/drive/My Drive/Colab Notebooks/Output/CNN/best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True) # change the monitor parameter to tune
mc = ModelCheckpoint('/content/drive/My Drive/Colab Notebooks/Output/CNN/best_modelTunORI.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True) # change the monitor parameter to tune
# fit model

from tensorflow.python.saved_model import loader_impl
from tensorflow.python.keras.saving.saved_model import load as saved_model_load

#change loss parameter to tune
#model.compile(optimizer='adam', loss='mse',metrics=['rmse','mape','mse','mae','accuracy']) #for rmse score
#model.compile(optimizer='adam', loss='mse',metrics=['mse','accuracy','mae'])
model.compile(optimizer='adam', loss='mse',metrics=['mse','mape','mae'])
# Compile model

# fit model
history = model.fit(X, y, validation_split=0.33, epochs=2000, verbose=1, callbacks=[es, mc]) #change the  validation_split from 0.50 to 0.10 tune
# load the saved model
#saved_model = load_model('best_model.h5', custom_objects={"rmse": rmse}) #for rmse score

saved_model = load_model('/content/drive/My Drive/Colab Notebooks/Output/CNN/best_modelTunORI.h5')

scores = saved_model.evaluate(X,y,verbose=0)
print((model.metrics_names, scores))
#print("%s: %.2f%%" % (model.metrics_names[1], scores[1]))



# demonstrate prediction
# input data is 365 days data and predicting next month data which is already mentioned in the above code
x_input = array(raw_seq[-365:])
x_input = x_input.reshape((1, n_steps_in, n_features))
yhat = saved_model.predict(x_input, verbose=0)

print(yhat)

#Creating output file for the predictions:

df.index = df["Date"]

df.index = pd.to_datetime(df.index)

lastDate = df.last_valid_index()

print(lastDate)
next = lastDate + pd.DateOffset(days=1)
print(next)

rng = pd.date_range(next, periods=31, freq='D') #only change periods in order to change number of prediction days ahead
idx = pd.Index(rng)

new = pd.DataFrame(yhat.reshape(-1, len(yhat)), columns=['Pred'], index= idx)

model.summary()

new.plot(figsize=(12, 8))

#output file
new.to_excel(r"/content/drive/My Drive/Colab Notebooks/Output/CNN/CNNnewORI.xlsx")

#save the model
#import pickle
#filename = '/content/drive/My Drive/Colab Notebooks/Output/CNN/final_modelORI.sav'

saved_model = load_model('/content/drive/My Drive/Colab Notebooks/Output/CNN/best_modelORI.h5')
#pickle.dump(saved_model, open(filename, 'wb'))



"""

---



# **RUN saved model**

---

"""

from google.colab import drive
drive.mount('/content/drive')

import pickle
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import datetime
# load all data
from keras.callbacks import ModelCheckpoint
# univariate multi-step vector-output 1d cnn model
from numpy import array
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.callbacks import EarlyStopping
from keras.models import load_model
# save the model to disk (not compulsary)
# split into samples
# split a univariate sequence into samples
def split_sequence(sequence, n_steps_in, n_steps_out):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        # check if we are beyond the sequence
        if out_end_ix > len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)


#input file
df = pd.read_csv(r'/content/KMJ4Daily2018-2020working.csv') #change the path to your drive

df = df.dropna()
raw_seq = df['Power Generation'].tolist()
n_steps_in, n_steps_out =365,31
n_features=1
X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)

X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model


#filename = '/content/drive/My Drive/Colab Notebooks/Output/CNN/final_modelORI.sav'


x_input = array(raw_seq[-365:])
x_input = x_input.reshape((1, n_steps_in, n_features))

# load the model from disk
#loaded_model = pickle.load(open(filename, 'rb'))
loaded_model=load_model("/content/best_modelTunORI2021.h5") #change the path to your drive
result = loaded_model.predict(x_input, verbose=0)
#Creating output file for the predictions:

df.index = df["Date"]

df.index = pd.to_datetime(df.index)

lastDate = df.last_valid_index()


next = lastDate + pd.DateOffset(days=1)


rng = pd.date_range(next, periods=31, freq='D') #only change periods in order to change number of prediction days ahead
idx = pd.Index(rng)

print(result)

new1 = pd.DataFrame(result.reshape(-1, len(result)), columns=['Pred'], index= idx)

#output file
new1.to_excel(r"/content/drive/My Drive/Colab Notebooks/Output/CNN/CNNSavedModelORI.xlsx")

scores = saved_model.evaluate(X,y,verbose=0)
print((model.metrics_names, scores))
#print("%s: %.2f%%" % (model.metrics_names[1], scores[1]))

new1.plot(figsize=(12, 8))